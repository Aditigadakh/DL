Build the Image classification model (CNN) by dividing the model into following 4 stages:
a. Loading and pre-processing the image data
b. Defining the model’s architecture
c. Training the model
d. Estimating the model’s performance
Dataset: CIFAR10
Class (“airplane”, “automobile”,” bird”, “cat”, “deer”,” frog”,” horse”,” ship”,” truck”)




import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D
import matplotlib.pyplot as plt
import numpy as np

# a. Loading and preprocessing image data
cifar10 = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
input_shape = (32, 32, 3)  # CIFAR-10 images are 32x32 pixels with 3 color channels

# Making sure that values are float so that we can get decimal points after division
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# Normalizing the RGB codes by dividing by the maximum RGB value (255)
x_train /= 255
x_test /= 255

# b. Defining the model architecture
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), input_shape=input_shape, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))  # Adjust the dropout rate as needed
model.add(Dense(10, activation='softmax'))

model.summary()

# c. Training the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10)  # Adjust the number of epochs as needed

# d. Estimating the model's performance
test_loss, test_acc = model.evaluate(x_test, y_test)
print("Loss = {:.3f}".format(test_loss))
print("Accuracy = {:.3f}".format(test_acc))

# Showing an image from the dataset
sample_image = x_train[0]
plt.imshow(sample_image)
plt.show()

# Predicting the class of the image
sample_image = sample_image.reshape(1, input_shape[0], input_shape[1], input_shape[2])
predictions = model.predict(sample_image)
predicted_class_index = np.argmax(predictions)
classes = ["airplane", "automobile", "bird", "cat", "deer", "frog", "horse", "ship", "truck"]
predicted_class = classes[predicted_class_index]
print("Predicted Class: {}".format(predicted_class))



# Predict the class of the sample image
sample_image = sample_image.reshape(1, 32, 32, 3)
predictions = model.predict(sample_image)
predicted_class = np.argmax(predictions)
print("Predicted Class: {}".format(predicted_class))


plt.rcParams['figure.figsize'] = [8, 8]  # Adjust the figure size
plt.rcParams['image.cmap'] = 'viridis'   # Use a different color map for better contrast


num_rows = 2
num_columns = 5  # Change this to the desired number of columns
# Create a figure with a grid of subplots
fig, axes = plt.subplots(num_rows, num_columns, figsize=(10, 4))  # Adjust the figure size as needed

# Display multiple sample images
for i in range(num_rows):
    for j in range(num_columns):
        index = i * num_columns + j
        sample_image = x_train[index]
        axes[i, j].imshow(sample_image)
        axes[i, j].set_title("Class: {}".format(y_train[index][0]))

# Adjust spacing and display the plot
plt.tight_layout()
plt.show()

# Predict the class of the sample image
# below code keep if required
# sample_image = sample_image.reshape(1, 32, 32, 3)
# predictions = model.predict(sample_image)
# predicted_class = np.argmax(predictions)
# predicted_class_name = class_names[predicted_class]

# print("Predicted Class: {}".format(predicted_class_name))

# The CIFAR-10 dataset contains class labels encoded as integers from 0 to 9.







#optional

from keras.datasets import cifar10 # This line imports the CIFAR-10 dataset from Keras, which is a popular dataset for image classification tasks. CIFAR-10 contains 60,000 32x32 color images in 10 different classes, with 6,000 images per class.

from matplotlib import pyplot # Imports the pyplot module from the Matplotlib library, which is used for creating plots and displaying images.

(train_X, train_y), (test_X, test_y) = cifar10.load_data() # This line loads the CIFAR-10 dataset and splits it into training and testing sets. train_X contains the training images, train_y contains the corresponding training labels, test_X contains the testing images, and test_y contains the testing labels.


print('X_train: ' + str(train_X.shape)) # This line prints the shape of the training data, which is the dimensions of the train_X array. It shows the number of training examples, image height, image width, and the number of color channels (3 for RGB).

print('Y_train: ' + str(train_y.shape)) # This line prints the shape of the training labels, which indicates the number of training examples.

print('X_test: ' + str(test_X.shape)) # This line prints the shape of the testing data, similar to what was done for the training data.

print('Y_test: ' + str(test_y.shape)) # This line prints the shape of the testing labels.

# Output --> X_train: (50000, 32, 32, 3)
#            Y_train: (50000, 1)
#            X_test: (10000, 32, 32, 3)
#            Y_test: (10000, 1)

# X_train: (50000, 32, 32, 3): This line indicates that the training data (X_train) has a shape of (50000, 32, 32, 3).
# This means:
# 50000: There are 50,000 training examples in the dataset.
# 32: Each image in the training set has a height of 32 pixels.
# 32: Each image in the training set has a width of 32 pixels.
# 3: There are 3 color channels per pixel, indicating that the images are in RGB (Red, Green, Blue) format.

# Y_train: (50000, 1): This line indicates that the training labels (Y_train) have a shape of (50000, 1).
# This means:
# 50000: There are 50,000 corresponding labels for the training examples.
# 1: Each label is a single scalar value, indicating the class of the corresponding image. In this context, CIFAR-10 is a classification dataset with 10 different classes (e.g., "airplane," "automobile," "bird," etc.), and each label represents one of these classes.

# X_test: (10000, 32, 32, 3): This line indicates that the testing data (X_test) has a shape of (10000, 32, 32, 3). This is similar to the training data but corresponds to the testing set, which contains different examples.

# Y_test: (10000, 1): This line indicates that the testing labels (Y_test) have a shape of (10000, 1). Like the training labels, there are 10,000 labels for the testing examples, with each label representing the class of the corresponding image.

# In summary, the output provides information about the dimensions and shapes of the training and testing data and labels in the CIFAR-10 dataset. The training set has 50,000 images, while the testing set has 10,000 images, and each image is a 32x32 pixel RGB image. The labels for both sets are represented as single scalar values.


for i in range(9):  # This line starts a loop that iterates 9 times, for the purpose of displaying the first 9 images from the training dataset.
    pyplot.subplot(330 + 1 + i) # This line sets up a subplot grid with 3 rows and 3 columns (3x3 grid) and selects the i-th subplot for displaying the image. The i variable iterates from 0 to 8, so this code selects the first 9 subplots in the grid.

    pyplot.imshow(train_X[i], cmap=pyplot.get_cmap('gray')) # This line displays the i-th image from the training dataset using Matplotlib's imshow function. It specifies the colormap as 'gray', which is used to display a color image in grayscale. The train_X[i] is the image to be displayed.
pyplot.show()




# The output you provided is from training a neural network on the CIFAR-10 dataset. Here's an explanation of the key parts of the output:

# Optimization Information:

# The output begins with a message related to TensorFlow's CPU feature optimization, indicating that the binary is optimized to use certain CPU instructions for performance-critical operations.
# Training Progress:

# It shows the progress of training the neural network over 10 epochs. Each epoch represents one complete pass through the entire training dataset.

# For each epoch, you can see information such as:

# Epoch X/10: Indicates the current epoch out of the total 10 epochs.
# loss: The training loss, which measures how well the model is fitting the training data. It starts relatively high and decreases over time.
# accuracy: The training accuracy, which represents the proportion of correctly classified training examples. It starts low and increases.
# Similarly, there's validation data performance:

# val_loss: The validation loss, which measures how well the model generalizes to data it hasn't seen during training. It may vary compared to the training loss.
# val_accuracy: The validation accuracy, which represents the accuracy on a separate validation dataset. It is used to monitor how well the model generalizes to new data.
# Test Accuracy:

# After training for 10 epochs, the code evaluates the model on the test dataset and prints the test accuracy, which represents how well the model performs on unseen data. In this case, the test accuracy is 46.60%, meaning the model correctly classifies 46.60% of the test examples.
# In summary, the output provides insight into the training and evaluation process of the neural network on the CIFAR-10 dataset, including information about loss and accuracy over multiple epochs and the final test accuracy. The goal is typically to train the model until the training and validation accuracies converge while avoiding overfitting to achieve good performance on unseen data.

# Training Progress:

# Epoch 1/10: This line indicates the start of the first training epoch out of a total of 10 epochs. Each epoch represents one pass through the entire training dataset.

# 1563/1563 [==============================] - 4s 3ms/step - loss: 1.9228 - accuracy: 0.3078 - val_loss: 1.7984 - val_accuracy: 0.3530: This line represents the progress and results of the first epoch. Here's a breakdown:

# 1563/1563: Indicates the batch progress. In this case, 1563 batches have been processed out of a total of 1563.

# [==============================]: The progress bar, which is filled as the training progresses through batches.

# - 4s 3ms/step: Indicates that the first epoch took 4 seconds to complete and processed batches at a rate of 3 milliseconds per batch.

# loss: 1.9228: The training loss at the end of the first epoch. This value indicates how well the model is fitting the training data. It's relatively high at the start and will hopefully decrease with more training.

# accuracy: 0.3078: The training accuracy at the end of the first epoch. This value represents the proportion of correctly classified training examples. It starts at 30.78%.

# val_loss: 1.7984: The validation loss at the end of the first epoch. This value measures how well the model generalizes to data it hasn't seen during training. It's important to monitor to detect overfitting.

# val_accuracy: 0.3530: The validation accuracy at the end of the first epoch. This value represents the accuracy on a separate validation dataset, which helps monitor how well the model generalizes to new data. It's initially at 35.30%.

# Repeating for Subsequent Epochs:

# The same format is repeated for the remaining epochs (2/10 through 10/10). Training and validation loss and accuracy are shown for each epoch, allowing you to see how the model's performance changes over time.
# Test Accuracy:

# Test accuracy: 46.60%: After training for 10 epochs, the code evaluates the model on the test dataset and prints the final test accuracy. The test accuracy represents how well the model performs on unseen data, and in this case, it's 46.60%.

# Q: What is Keras, and how does it relate to TensorFlow?
# A: Keras is an open-source high-level neural networks API that runs on top of TensorFlow, providing an easier and more user-friendly way to define and train neural networks.

# Q: Why do we need to import packages for this task?
# A: Importing packages like TensorFlow and Keras is essential to access the necessary functions and tools for building and training neural networks.

# Q: How can you load the MNIST dataset for training and testing?
# A: You can load the MNIST dataset using tf.keras.datasets.mnist.load_data().

# Q: What is the structure of the MNIST dataset?
# A: The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9) and their corresponding labels.

# Q: Explain the architecture of a feedforward neural network.
# A: A feedforward neural network consists of an input layer, one or more hidden layers, and an output layer. Each layer contains multiple neurons that process and transform data.

# Q: How do you define the network architecture in Keras?
# A: You can define the architecture using the Keras Sequential API, where you stack layers using model.add().

# Q: What is Stochastic Gradient Descent (SGD) and why is it used for training?
# A: SGD is an optimization algorithm that updates model weights using a random subset (mini-batch) of the training data. It's used for training to optimize the model's parameters gradually.

# Q: How do you compile a Keras model for training with SGD?
# A: You use the model.compile() method and specify the loss function, optimizer (SGD in this case), and evaluation metrics.

# Q: What is a loss function, and why is it important in training neural networks?
# A: The loss function quantifies the model's performance by measuring the difference between predicted and actual values. It's crucial for guiding weight updates during training.

# Q: How do you evaluate a neural network model after training?
# A: You can use the model.evaluate() method to evaluate the model's performance on a separate test dataset.

# Q: What metrics can be used to evaluate the network's performance?
# A: Common metrics include accuracy, loss, precision, recall, and F1-score, depending on the problem type (classification or regression).

# Q: Why is it important to visualize the training loss and accuracy?
# A: Visualizing the training loss and accuracy helps in understanding the model's learning progress, identifying overfitting, and making necessary adjustments.

# Q: How can you plot the training loss and accuracy in Keras?
# A: You can use libraries like Matplotlib to plot the training loss and accuracy using the history object returned by model.fit().

# Q: What is overfitting, and how can it be detected in training curves?
# A: Overfitting occurs when a model performs well on the training data but poorly on unseen data. It can be detected by a significant gap between training and validation (or test) loss curves.

# Q: Can you explain the concept of learning rate in SGD?
# A: The learning rate in SGD determines the size of the steps taken during weight updates. It's a hyperparameter that affects the convergence and stability of training.

# Q: What are the potential challenges in training a neural network?
# A: Challenges include selecting appropriate hyperparameters, handling data preprocessing, avoiding overfitting, and dealing with vanishing/exploding gradients.

# Q: How can you improve the performance of a feedforward neural network?
# A: Performance can be improved by tuning hyperparameters, increasing model complexity, using different optimizers, and applying data augmentation.

# Q: What is the purpose of the activation functions in neural network layers?
# A: Activation functions introduce non-linearity to the network, allowing it to learn complex patterns and relationships within the data.

# Q: Can you briefly explain the backpropagation algorithm?
# A: Backpropagation is the process of calculating gradients for network weights by propagating error gradients backward through the network from the output to the input.

# Q: How can you handle imbalanced classes in a classification problem?
# A: Techniques for handling imbalanced classes include oversampling, undersampling, using different evaluation metrics, and incorporating class weights during training.
	